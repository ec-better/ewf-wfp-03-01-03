#!/opt/anaconda/envs/env_ewf_wfp_03_01_03/bin/python
#########
## This node recieves a comma seperated list of input references - various tiles 
## GDAL opens and get the GeoTransform data per tile
## Groups tiles according to geometry and sort by date 
## publishes each group as a pickle file containing panda DataFrame 
#########
import os
import sys
import string
import atexit
import gdal
gdal.UseExceptions()

import cioppy 
ciop = cioppy.Cioppy()
import pandas as pd
sys.path.append('/'.join([os.environ['_CIOP_APPLICATION_PATH'], 'util']))
sys.path.append('../util')
from s3_whittaker_helpers import *
# define the exit codes
SUCCESS = 0
ERR_RESOLUTION = 10
ERR_STAGEIN = 20
ERR_NO_OUTPUT = 30

# add a trap to exit gracefully
def clean_exit(exit_code):
    log_level = 'INFO'
    if exit_code != SUCCESS:
        log_level = 'ERROR'  
   
    msg = {SUCCESS: 'Processing successfully concluded',
           ERR_RESOLUTION: 'Could not resolve Sentinel-1 product enclosure',
           ERR_STAGEIN: 'Could not stage-in Sentinel-1 product', 
           ERR_NO_OUTPUT: "Missing output"
    }
 
    ciop.log(log_level, msg[exit_code])  

def main():
    ciop = cioppy.Cioppy()
    
    data_pipeline_parameters = dict()
    
    data_pipeline_parameters['username'] = ciop.getparam('_T2Username')
    data_pipeline_parameters['api_key'] = ciop.getparam('_T2ApiKey')

    data_pipeline_parameters['input_endpoint'] = ciop.getparam('input_endpoint')
    
    #### Search Params
 
    search_params = dict()
    
    search_params['start'] = ciop.getparam('series_startdate')
    search_params['stop'] = ciop.getparam('series_enddate')
    search_params['q'] = 'S3*_OL_2_LFR____*'
    tile_column =  ciop.getparam('tile_column')
    tile_row =  ciop.getparam('tile_row')
    tile_level =  ciop.getparam('tile_level')
    track =  ciop.getparam('track')
    
    
    search_params['cat'] =  '!dataitem'
    search_params['count'] = 'unlimited'
    
    
    creds = '{}:{}'.format(data_pipeline_parameters['username'],
                           data_pipeline_parameters['api_key'])

    search_input = pd.DataFrame(ciop.search(end_point=data_pipeline_parameters['input_endpoint'],
                                      params=search_params,
                                      output_fields='self,startdate,enclosure,title',
                                      model='GeoTime',
                                      timeout=1200000,
                                      creds=creds))
    
    
    search_input.columns=['self','startdate','enclosure','title']
    
    #Selecting the required tiles
    selected_tile = 'Tile L:{} C:{} R:{}'.format(tile_level,tile_column,tile_row)
    selected_track = '_{}_'.format(track)
    ref_dataframe = search_input[search_input.apply(lambda row: selected_tile in row['title'] and track in row['title'], axis=1)]
    ref_dataframe.reset_index(drop=True)
    
    ref_dataframe = ref_dataframe.drop_duplicates(subset=['startdate'], keep="first", inplace=False)
    
    # Removing properties file wrongly published
    ref_dataframe.drop(ref_dataframe[ref_dataframe.apply(lambda row: '.properties' in row['enclosure'], axis=1)].index, inplace=True)
    ref_dataframe = ref_dataframe.reset_index(drop=True)
    
    
    references = ref_dataframe['self'].tolist()
    
    ciop.log('INFO','Retrieved {} input tiles'.format(len(references)))    


    os.chdir(ciop.tmp_dir)
    ### Temporary value for test
    #path_to_store='https://store.terradue.com/ppishehvar/_results/workflows/ec_better_ewf_wfp_03_01_01_ewf_wfp_03_01_01_0_2/run/1e631064-b158-11ea-bbe6-0242ac110003/0013082-200518181542206-oozie-oozi-W/9a2364b0-51ef-4a2b-a397-a0e0c28459aa/'
    
    
    
    #references = []
        
    #for input in sys.stdin:
    #    ciop.log('INFO', 'Adding {}'.format(input.rstrip()))       
    #    references.append(input.rstrip().split(',')[0])
        
    #To be modified when input data pipeline is configured        
    data_pipeline_results = pd.DataFrame()
    input_tile = dict()
    for end_point in references:
        ciop.log('INFO', 'Getting metadata for {}'.format(end_point)) 
        input_tile['tile'] = end_point
        input_tile['title'] = ref_dataframe[ref_dataframe['self']==end_point]['title'].values[0]#end_point.split('.')[0]
        input_tile['startdate'] = ref_dataframe[ref_dataframe['self']==end_point]['startdate'].values[0]
        input_tile['enclosure'] = ref_dataframe[ref_dataframe['self']==end_point]['enclosure'].values[0]#'{}/{}'.format(path_to_store,end_point)
        
        vsi_url = get_vsi_url(input_tile['enclosure'],data_pipeline_parameters['username'],data_pipeline_parameters['api_key'])
        
        ds = gdal.Open(vsi_url)
        gt = ds.GetGeoTransform()
        input_tile['upper_left'] = gdal.ApplyGeoTransform(gt,0,0)
        input_tile['lower_right'] = gdal.ApplyGeoTransform(gt,ds.RasterXSize,ds.RasterYSize)
        
        data_pipeline_results = data_pipeline_results.append(input_tile, ignore_index=True)
       
    data_pipeline_results = data_pipeline_results.merge(data_pipeline_results.apply(lambda row: analyse_row(row), axis=1),
                                                        left_index=True,
                                                        right_index=True)




### Convert gps coordinates into comma seprated string to be inter-comparable 
    data_pipeline_results['Upper_Left'] = [','.join(map(str, l)) for l in data_pipeline_results['upper_left']]
    data_pipeline_results['Lower_Right'] = [','.join(map(str, l)) for l in data_pipeline_results['lower_right']]
    
    #origin_points = data_pipeline_results['upper_left'].unique().tolist()
    origin_points = data_pipeline_results['Upper_Left'].unique().tolist()
    
### Group tiles by geometry and sort by date
#    for origin in origin_points:
#        tile_stack=data_pipeline_results[data_pipeline_results['Upper_Left'] == origin].reset_index(drop=True)
#        if tile_stack['lower_right'].unique():
#            tile_stack.sort_values(by=['jday'],inplace=True, ignore_index=True)
#            tile_stack_name = tile_stack.iloc[0]['title']
#            tile_stack.to_pickle(os.path.join(ciop.tmp_dir,'{}.pickle'.format(tile_stack_name)), 'gzip')
#            ciop.log('INFO', 'Publish {}.pickle'.format(tile_stack_name))
#            for band in ['NDVI','OGVI', 'OTCI']:
#                ciop.publish(os.path.join(ciop.tmp_dir, '{0}-{1}.pickle'.format(band,tile_stack_name)))
    
### Group tiles by geometry and sort by date    
    for origin in origin_points:
        tile_stack=data_pipeline_results[data_pipeline_results['Upper_Left'] == origin].reset_index(drop=True)
        
        if tile_stack['Lower_Right'].unique():
            
            tile_stack.sort_values(by=['jday'],inplace=True, ignore_index=True)
            title_splitted = tile_stack.iloc[0]['title'].split(' ')
            title_splitted[-1].split('_')[0:4]
            tile_stack_name = '{}_track_{}_{}'.format('_'.join(title_splitted[-1].split('_')[0:4]),track,'_'.join(title_splitted[:4]))
            tile_stack_name = tile_stack_name.replace(':','')
         
            #tile_stack.to_pickle(os.path.join(ciop.tmp_dir,'{}.pickle'.format(tile_stack_name)), 'gzip')
            pkl_path = os.path.join(ciop.tmp_dir,'{}.pickle'.format(tile_stack_name))
            tile_stack.to_pickle(pkl_path, compression='gzip')
            
            # pickle to copied 3 times for the different bands
            os.move()
            
            for band in ['NDVI','OGVI', 'OTCI']:
                
                pkl_path = os.path.join(ciop.tmp_dir,'{}_{}.pickle'.format(band,tile_stack_name))
               
            
            
            ciop.log('INFO', 'Publish {}.pickle'.format(tile_stack_name))
            
            
                ciop.publish(os.path.join(ciop.tmp_dir, '{}_{}'.format(band,tile_stack_name)))



try:
    main()
except SystemExit as e:
    if e.args[0]:
        clean_exit(e.args[0])
    raise
else:
    atexit.register(clean_exit, 0)
